{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215c1423-7c16-4c01-8b6f-e3146850d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a639455-174f-4188-82e3-86fe599a4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import CLIP.clip as clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf4e42-6bc0-4273-a2c2-6212a106bcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2923bd41-8111-4a21-b8ea-4e4b18fc8120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def quantize_4b_codebook(tensor):\n",
    "    \n",
    "    # Create a codebook with 16 levels (4 bits)\n",
    "    n_levels = 2**6\n",
    "    min_val = torch.min(tensor)\n",
    "    max_val = torch.max(tensor)\n",
    "    \n",
    "    # Create codebook\n",
    "    codebook = torch.linspace(min_val, max_val, n_levels).to('cuda')\n",
    "    \n",
    "    # Reshape tensor and codebook for broadcasting\n",
    "    x_flat = tensor.reshape(-1, 1)\n",
    "    \n",
    "    # Find nearest codebook entry\n",
    "    distances = torch.abs(x_flat - codebook)\n",
    "    indices = torch.argmin(distances, dim=1)\n",
    "    return indices.reshape(tensor.shape), min_val,(max_val-min_val)/n_levels\n",
    "\n",
    "def dequantize_4b_codebook(indices, min_val , scale):\n",
    "    \"\"\"\n",
    "    Dequantize using codebook\n",
    "    \"\"\"\n",
    "    return min_val+indices*scale\n",
    "class SDV_FC(nn.Module):\n",
    "    def __init__(self, l1, l2, bias, remainder):\n",
    "        super(SDV_FC, self).__init__()\n",
    "        self.l1 = nn.Parameter(l1.clone())\n",
    "        self.l2 = nn.Parameter(l2.clone())\n",
    "        self.bias = nn.Parameter(bias.clone())\n",
    "        self.remainder,self.min_val , self.scale=quantize_4b_codebook(remainder)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Compute standard part\n",
    "        standard_out = x @ self.l2.t() @ self.l1.t()\n",
    "        \n",
    "    \n",
    "        # Compute remainder part\n",
    "        remainder_out = x @ dequantize_4b_codebook(self.remainder,self.min_val , self.scale).t()\n",
    "        \n",
    "        # Combine all parts\n",
    "        out = remainder_out + standard_out + self.bias\n",
    "        return out\n",
    "\n",
    "\n",
    "def SDV_FC_FullyConnectedLayers(fc_weight, fc_bias, rank=None):\n",
    "    # Compute SVD\n",
    "    U, S, Vh = torch.linalg.svd(fc_weight, full_matrices=False)\n",
    "    \n",
    "    # Verify S is sorted\n",
    "    assert torch.all(S[:-1] >= S[1:]), \"Singular values not sorted!\"\n",
    "    \n",
    "    # Take components corresponding to non-zero singular values\n",
    "    U_r = U[:, :rank]\n",
    "    S_r = S[:rank]\n",
    "    V_r = Vh[:rank, :]\n",
    "    \n",
    "    # Create scaled matrices\n",
    "    l1 = U_r @ torch.diag(S_r)\n",
    "    l2 = V_r\n",
    "    remainder = fc_weight - l1 @ l2\n",
    "    \n",
    "    return SDV_FC(l1, l2, fc_bias, remainder)\n",
    "\n",
    "def convert_sdv_quant(model: nn.Module):\n",
    "    def _convert_sdv(module: nn.Module):\n",
    "        for name, child in module.named_children():\n",
    "            if isinstance(child, nn.Linear):\n",
    "                r = 128\n",
    "                # Create the SDV_FC replacement\n",
    "                lora_layer = SDV_FC_FullyConnectedLayers(\n",
    "                    child.weight.data.float(),\n",
    "                    child.bias,\n",
    "                    r\n",
    "                )\n",
    "                module._modules[name] = lora_layer\n",
    "            _convert_sdv(child)\n",
    "        return module\n",
    "    \n",
    "    return _convert_sdv(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7956f437-a0b6-492a-b4a9-92aea8c8a8b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convert_sdv_quant(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b33a836-70c3-4bdb-9232-8323bc38e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.tokenize(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2a591-520a-4c47-acc3-d3c0b6aa2650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# images in skimage to use and their textual descriptions\n",
    "descriptions = {\n",
    "    \"page\": \"a page of text about segmentation\",\n",
    "    \"chelsea\": \"a facial photo of a tabby cat\",\n",
    "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
    "    \"rocket\": \"a rocket standing on a launchpad\",\n",
    "    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
    "    \"camera\": \"a person looking at a camera on a tripod\",\n",
    "    \"horse\": \"a black-and-white silhouette of a horse\", \n",
    "    \"coffee\": \"a cup of coffee on a saucer\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a71769f-4a69-4364-a1cb-b66f824a5f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = []\n",
    "images = []\n",
    "texts = []\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "for filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    if name not in descriptions:\n",
    "        continue\n",
    "\n",
    "    image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n",
    "  \n",
    "    plt.subplot(2, 4, len(images) + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"{filename}\\n{descriptions[name]}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    original_images.append(image)\n",
    "    images.append(preprocess(image))\n",
    "    texts.append(descriptions[name])\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca5c0a9-8592-48c7-b3ff-07c65506f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = torch.tensor(np.stack(images)).cuda()\n",
    "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e0218-3488-43fb-9c92-ef7e1a7524de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visual(image_input.type(model.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e8fc3-0a8a-4ded-b910-d2d3f76d956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input).float()\n",
    "    text_features = model.encode_text(text_tokens).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a656e8bb-2204-4dd0-805a-df53f786c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95403be-90cf-44c5-b18a-c3a73d2c8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = len(descriptions)\n",
    "\n",
    "plt.figure(figsize=(20, 14))\n",
    "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
    "# plt.colorbar()\n",
    "plt.yticks(range(count), texts, fontsize=18)\n",
    "plt.xticks([])\n",
    "for i, image in enumerate(original_images):\n",
    "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
    "for x in range(similarity.shape[1]):\n",
    "    for y in range(similarity.shape[0]):\n",
    "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
    "\n",
    "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "  plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "plt.xlim([-0.5, count - 0.5])\n",
    "plt.ylim([count + 0.5, -2])\n",
    "\n",
    "plt.title(\"Cosine similarity between text and image features\", size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ca160-0882-4e8a-baea-bd598d0c48e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "cifar100 = CIFAR100(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d7916f-128c-437a-98b5-daf3918b61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_descriptions = [f\"This is a photo of a {label}\" for label in cifar100.classes]\n",
    "text_tokens = clip.tokenize(text_descriptions).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a18b48-4a29-4055-97aa-c3b092111671",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens).float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3958a2c-43e3-405a-9d19-c2053fd7cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "for i, image in enumerate(original_images):\n",
    "    plt.subplot(4, 4, 2 * i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(4, 4, 2 * i + 2)\n",
    "    y = np.arange(top_probs.shape[-1])\n",
    "    plt.grid()\n",
    "    plt.barh(y, top_probs[i])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_axisbelow(True)\n",
    "    plt.yticks(y, [cifar100.classes[index] for index in top_labels[i].numpy()])\n",
    "    plt.xlabel(\"probability\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dbb051-c5d3-442d-a239-327d1c2a9564",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cifar100.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c4cd2-3d49-483c-a1a6-9b7293d78392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "device=\"cuda\"\n",
    "text_descriptions = [f\"a photo of a {label}\" for label in cifar100.classes]\n",
    "text_tokens = clip.tokenize(text_descriptions).to(device)\n",
    "\n",
    "# Encode text prompts once (and normalize)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens).float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Create DataLoader for CIFAR-100 (using a batch size that fits your GPU memory)\n",
    "batch_size = 64  # Adjust batch size as needed\n",
    "subset_indices = list(range(1024))\n",
    "cifar100_subset = Subset(cifar100, subset_indices)\n",
    "dataloader = DataLoader(cifar100_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Variables to accumulate correct predictions and total count\n",
    "top1_correct = 0\n",
    "top5_correct = 0\n",
    "total_samples = 0\n",
    "start=time.time()\n",
    "# Iterate over the dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        total_samples += images.size(0)\n",
    "        \n",
    "        # Encode images using CLIP\n",
    "        image_features = model.encode_image(images).float()\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Compute the cosine similarity between image features and text features,\n",
    "        # scaled by a temperature factor (here 100.0, as in the original CLIP paper)\n",
    "        logits = 100.0 * image_features @ text_features.T\n",
    "        \n",
    "        # Convert logits to probabilities (optional for accuracy calculation)\n",
    "        probs = logits.softmax(dim=-1)\n",
    "        \n",
    "        # Get the indices of the top 5 predictions\n",
    "        top5_probs, top5_indices = probs.topk(5, dim=-1)\n",
    "        \n",
    "        # Top-1 prediction is the first element of top5_indices\n",
    "        top1_predictions = top5_indices[:, 0]\n",
    "        \n",
    "        # Update top-1 correct count: compare the top1 prediction with the true label\n",
    "        top1_correct += (top1_predictions == labels).sum().item()\n",
    "        \n",
    "        # Update top-5 correct count: check if true label is within the top 5 predictions\n",
    "        # For each sample, this returns a boolean tensor that we sum up.\n",
    "        top5_correct += sum([label in top5_indices[i] for i, label in enumerate(labels)])     \n",
    "# Compute accuracies\n",
    "print(\"time\" , time.time()-start)\n",
    "top1_accuracy = top1_correct / total_samples\n",
    "top5_accuracy = top5_correct / total_samples\n",
    "\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(f\"Top-1 Accuracy: {top1_accuracy * 100:.2f}%\")\n",
    "print(f\"Top-5 Accuracy: {top5_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a588db7c-b871-4dcc-be7f-4775b2a36afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424bb11-c79c-44d0-b9af-8b34c03eb2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9056d-e467-45e3-a628-5004121ad5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visual.transformer.resblocks[0].attn.q_proj.remainder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4b312-3abd-49f6-ac17-9c69f411b65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9a118-3d8c-4424-8d6b-6995954b0298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
